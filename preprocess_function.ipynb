{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqbJGP4eJjXD"
   },
   "source": [
    "# Functions for preprocessing data\n",
    "below are my helper functions created to preprocess my data to feed into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11rvkHefW1WQ"
   },
   "source": [
    "## Sub functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h77PZuz_xVt6"
   },
   "source": [
    "### `csv_to_dataframe`\n",
    "takes in a variable number of csv file paths and outputs an array of pandas dataframes and a list of unique bookingIDs for identification in later functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDwufvcHJetW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def csv_to_dataframe(*args):\n",
    "    unique_IDs = []\n",
    "    dataframes = []\n",
    "    for i in args:\n",
    "        temp_df = pd.read_csv(i, delimiter = \",\")\n",
    "        temp_df.sort_values(by=[\"bookingID\",\"second\"])\n",
    "\n",
    "        temp_df = temp_df.drop(columns =[\"Accuracy\", \"Bearing\", \"Speed\", \"gyro_x\", \"gyro_y\", \"gyro_z\"])\n",
    "        # Rationale: might be able to tell with just accelerometer data. \n",
    "\n",
    "        temp_unique = temp_df.bookingID.unique()\n",
    "        unique_IDs.append(temp_unique)\n",
    "        dataframes.append(temp_df)\n",
    "    return dataframes, unique_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyBmWx3_xmNF"
   },
   "source": [
    "### `preprocess_split_df`\n",
    "takes in an array of pandas dataframes, a list of unique bookingIDs and outputs a list where each element is a dataframe with the same bookingID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61GXUFsQJoyh"
   },
   "outputs": [],
   "source": [
    "def preprocess_split_df(dataframe, unique_ids): \n",
    "    data = []\n",
    "    for id in unique_ids:\n",
    "        temp = dataframe.loc[dataframe.bookingID == id]\n",
    "        data.append(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCo9SXBby0gd"
   },
   "source": [
    "### `preprocess_match_label`\n",
    "Takes in a dataframe array. Then matches corresponding labels (0 or 1) with each dataframe in a dataframe array based on bookingID. Also drops bookingID column once data is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AkQ5BCpKcC6"
   },
   "outputs": [],
   "source": [
    "def preprocess_match_label(dataframes, dataframe_labels):\n",
    "    output = []\n",
    "    for dataframe in dataframes:\n",
    "    \n",
    "        # start matching against dataframe\n",
    "        bookingID = dataframe[\"bookingID\"].iloc[0]\n",
    "        dataframe = dataframe.drop(columns = \"bookingID\")\n",
    "    \n",
    "        for labels in dataframe_labels.values:\n",
    "            if(labels[0] == bookingID):\n",
    "                output.append([dataframe, labels[1]])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnUJ29JNzQxG"
   },
   "source": [
    "### `preprocess_convert_df_to_list`\n",
    "takes in an array of `(dataframe, label)` pairs and converts them to a list of `(numpy_array, label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFjeYX55LoBw"
   },
   "outputs": [],
   "source": [
    "def preprocess_convert_df_to_list(data):\n",
    "    output = []\n",
    "    for dataframe, target in data:\n",
    "        output.append([np.asarray(dataframe.values), target]) # editing data inplace, faster but less room for error...\n",
    "  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4C-o983ztFN"
   },
   "source": [
    "### `preprocess_pass_or_fail`\n",
    "balances number of dangerous and non-dangerous driving cases for unbiased training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJKMw4nSLonF"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def preprocess_pass_or_fail(data):\n",
    "\n",
    "    not_dangerous = []\n",
    "    dangerous = []\n",
    "  \n",
    "    for sequence, label in data:\n",
    "        if label == 0:\n",
    "            not_dangerous.append([sequence, label])\n",
    "        elif label == 1:\n",
    "            dangerous.append([sequence, label])\n",
    "\n",
    "    # balance data\n",
    "    lower = min(len(not_dangerous), len(dangerous))\n",
    "  \n",
    "    not_dangerous = not_dangerous[:lower]\n",
    "    dangerous = dangerous[:lower]\n",
    "  \n",
    "    print(f'number of dangerous driving cases: {len(dangerous)}\\nnumber of safe driving cases: {len(not_dangerous)}\\n')\n",
    "  \n",
    "    balanced_data = not_dangerous + dangerous\n",
    "    random.shuffle(balanced_data)\n",
    "    \n",
    "    x = [] # sequential data\n",
    "    y = [] # labels\n",
    "  \n",
    "    for sequence, labels in balanced_data:\n",
    "        x.append(sequence)\n",
    "        y.append(labels)\n",
    "    \n",
    "    return np.array(x), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbWKT2ETXBCq"
   },
   "source": [
    "## Import csv files\n",
    "**Note to reviewer:** ensure the csv files are in the appropriate locations OR alter the filepath for the labels in the body of the function `preprocess_data_for_training` and the filepaths given in its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "YRq1pNeqLsHh",
    "outputId": "1c8506e4-57d9-4e1c-b0a8-cde3e9c8ff3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://grab-ai-for-sea/features/.DS_Store...\n",
      "Copying gs://grab-ai-for-sea/features/part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00001-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00002-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "| [4 files][567.2 MiB/567.2 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://grab-ai-for-sea/features/part-00003-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00004-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00005-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00006-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00007-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00008-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "Copying gs://grab-ai-for-sea/features/part-00009-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv...\n",
      "| [11 files][  1.9 GiB/  1.9 GiB]  113.4 MiB/s                                  \n",
      "Operation completed over 11 objects/1.9 GiB.                                     \n",
      "Copying gs://grab-ai-for-sea/labels/.DS_Store...\n",
      "Copying gs://grab-ai-for-sea/labels/part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv...\n",
      "/ [2 files][304.3 KiB/304.3 KiB]                                                \n",
      "Operation completed over 2 objects/304.3 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# importing data from google cloud storage (GCS)\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "project_id='My First Project'\n",
    "!gsutil cp -r gs://grab-ai-for-sea/features /content/\n",
    "!gsutil cp -r gs://grab-ai-for-sea/labels /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKd0ql_IXEp5"
   },
   "source": [
    "## Main function using sub functions\n",
    "Runs all the subfunctions above in sequence. Returns a numpy array where each element is a sequence of data from the same booking ID and another array of labels.\n",
    "\n",
    "**Note:** Change the location for the labels if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "M3UcA1TML6g0",
    "outputId": "bd8b55ce-3af1-4bf2-97b3-df332978e883"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess_data_for_training(*args):\n",
    "    print(\"converting csv to dataframes.....\", end = '')\n",
    "    dataframe_array, unique_id_array = csv_to_dataframe(*args)\n",
    "    arg_count = 0\n",
    "    for i in args:\n",
    "        arg_count += 1\n",
    "    print(\"done.\")\n",
    "  \n",
    "    print(\"splitting dataframes.....\", end = '')\n",
    "    datas = []\n",
    "    for i in range(arg_count):\n",
    "        temp = preprocess_split_df(dataframe_array[i], unique_id_array[i])\n",
    "        datas.append(temp)\n",
    "    print(\"done.\")\n",
    "\n",
    "    print(\"matching dataframes and labels.....\", end = '')\n",
    "    # matching labels\n",
    "    df_labels = pd.read_csv(\"/content/labels/part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv\")\n",
    "    df_and_labels = []\n",
    "    for i in range(arg_count):\n",
    "        temp = preprocess_match_label(datas[i], df_labels)\n",
    "        df_and_labels.append(temp)\n",
    "    print(\"done.\")\n",
    "  \n",
    "    print(\"converting dataframes to lists.....\", end = '')\n",
    "    list_and_labels = []\n",
    "    for i in range(arg_count):\n",
    "        temp = preprocess_convert_df_to_list(df_and_labels[i])\n",
    "        list_and_labels.append(temp)\n",
    "    print(\"done.\")\n",
    "  \n",
    "  \n",
    "    print(\"balancing safe and dangerous datasets.....\\n\", end = '')\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "\n",
    "    for i in range(arg_count):\n",
    "        temp_x, temp_y = preprocess_pass_or_fail(list_and_labels[i])\n",
    "        x_array.append(temp_x)\n",
    "        y_array.append(temp_y)\n",
    "    print(\"done.\")\n",
    "    \n",
    "    print(\"concatenating and padding return values.....\", end = '')\n",
    "    # join and pad data\n",
    "    concat_values = np.concatenate(x_array)  \n",
    "    concat_values = pad_sequences(concat_values, dtype='float32')\n",
    "\n",
    "    concat_labels = np.concatenate(y_array)\n",
    "    print(\"done.\")\n",
    "\n",
    "    return concat_values, concat_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the preprocessing function\n",
    "run the main function on all the available csv files to compile their data into a list of sequences and a list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "colab_type": "code",
    "id": "oXrlRLSSMa_a",
    "outputId": "2360c369-c84f-41fd-d172-40d45709f994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting csv to dataframes.....done.\n",
      "splitting dataframes.....done.\n",
      "matching dataframes and labels.....done.\n",
      "converting dataframes to lists.....done.\n",
      "balancing safe and dangerous datasets.....\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "done.\n",
      "concatenating and padding return values.....done.\n"
     ]
    }
   ],
   "source": [
    "x_values, y_values = preprocess_data_for_training(\"/content/features/part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00001-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00002-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00003-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00004-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00005-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00006-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00007-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00008-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\",\n",
    "                                                  \"/content/features/part-00009-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "dhQYvlnwRqWN",
    "outputId": "5d82e8c0-b19f-4575-d9f2-dbc4e910f6a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100020, 778, 4)\n",
      "(100020,)\n"
     ]
    }
   ],
   "source": [
    "# double check the shape of the data.\n",
    "print(x_values.shape) # indicates that we have 100020 sequences which are each 778 rows long with 4 columns per row.\n",
    "print(y_values.shape) # only the labels, thats why it is one dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nxmdb1MvXXje"
   },
   "source": [
    "## Saving processed data into pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ansXgeltWwzo"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"preprocessedAccel_Sequences.pickle\", \"wb\")\n",
    "pickle.dump(x_values, pickle_out)\n",
    "\n",
    "pickle_out_y = open(\"preprocessedAccel_Labels.pickle\", \"wb\")\n",
    "pickle.dump(y_values, pickle_out_y)\n",
    "\n",
    "pickle_out.close()\n",
    "pickle_out_y.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6jG-E9LXpje"
   },
   "source": [
    "### Saving pickles to GCS\n",
    "**Note to reviewer:** use whichever method you prefer to store the pickles. Just make sure they are imported appropriately in the other jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "GhPHdZy9Xfsb",
    "outputId": "4bdabbbe-f40b-4804-8e7a-cb0cb7751dba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/preprocessedAccel_Sequences.pickle [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/  1.2 GiB]                                                \r",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [1 files][  1.2 GiB/  1.2 GiB]  100.3 MiB/s                                   \n",
      "Operation completed over 1 objects/1.2 GiB.                                      \n",
      "Copying file:///content/preprocessedAccel_Labels.pickle [Content-Type=application/octet-stream]...\n",
      "/ [1 files][781.6 KiB/781.6 KiB]                                                \n",
      "Operation completed over 1 objects/781.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/preprocessedAccel_Sequences.pickle gs://grab-ai-for-sea\n",
    "!gsutil cp /content/preprocessedAccel_Labels.pickle gs://grab-ai-for-sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iikokKJfVdp"
   },
   "source": [
    "# Preprocessing test data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "TdkAy7Eoxy6O",
    "outputId": "7d50b4bc-76e8-443b-eaa9-57ecd1e3cb98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting csv to dataframes.....done.\n",
      "splitting dataframes.....done.\n",
      "matching dataframes and labels.....done.\n",
      "converting dataframes to lists.....done.\n",
      "balancing safe and dangerous datasets.....\n",
      "number of dangerous driving cases: 5001\n",
      "number of safe driving cases: 5001\n",
      "\n",
      "done.\n",
      "concatenating and padding return values.....done.\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = preprocess_data_for_training(\"/content/features/part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Tajr0iOufy7o",
    "outputId": "ef8f1519-3671-4104-da94-3713a159675a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10002, 750, 4)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape) # make sure shape is different from 778, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BYvaOcjOXOe"
   },
   "source": [
    "Saving test sequence as pickle. To import when testing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "woX5VTVig0ck",
    "outputId": "6217ff70-ce3a-4f77-f233-3c1e42aa026e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/testData_Sequences.pickle [Content-Type=application/octet-stream]...\n",
      "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
      "Operation completed over 1 objects/114.5 MiB.                                    \n",
      "Copying file:///content/testData_Labels.pickle [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 78.3 KiB/ 78.3 KiB]                                                \n",
      "Operation completed over 1 objects/78.3 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "pickle_out = open(\"testData_Sequences.pickle\", \"wb\")\n",
    "pickle.dump(test_x, pickle_out)\n",
    "\n",
    "pickle_out_y = open(\"testData_Labels.pickle\", \"wb\")\n",
    "pickle.dump(test_y, pickle_out_y)\n",
    "\n",
    "pickle_out.close()\n",
    "pickle_out_y.close()\n",
    "\n",
    "# save the test sequences for later\n",
    "!gsutil cp /content/testData_Sequences.pickle gs://grab-ai-for-sea\n",
    "!gsutil cp /content/testData_Labels.pickle gs://grab-ai-for-sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEfyIB5yOhYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "preprocess_function.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
