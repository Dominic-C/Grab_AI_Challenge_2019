{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "JFsZkl0blWsw",
    "outputId": "ae736dd6-cdc5-4c23-8926-45ad1e9a3c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://grab-ai-for-sea/preprocessedAccel_Sequences.pickle...\n",
      "/ [1 files][  1.2 GiB/  1.2 GiB]  101.3 MiB/s                                   \n",
      "Operation completed over 1 objects/1.2 GiB.                                      \n",
      "Copying gs://grab-ai-for-sea/preprocessedAccel_Labels.pickle...\n",
      "/ [1 files][781.6 KiB/781.6 KiB]                                                \n",
      "Operation completed over 1 objects/781.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "project_id='My First Project'\n",
    "!gsutil cp -r gs://grab-ai-for-sea/preprocessedAccel_Sequences.pickle /content/\n",
    "!gsutil cp -r gs://grab-ai-for-sea/preprocessedAccel_Labels.pickle /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvDdRrk9lkwl"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"preprocessedAccel_Sequences.pickle\", \"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in_y = open(\"preprocessedAccel_Labels.pickle\", \"rb\")\n",
    "labels = pickle.load(pickle_in_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "WmXA9hIRluLS",
    "outputId": "d2b9b62e-f7dc-4fa1-a8cf-bee3263cdfb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100020, 778, 4)\n",
      "(100020,)\n",
      "80016 20003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "split = 0.8\n",
    "\n",
    "train_x = data[:int(float(len(data))*split)]\n",
    "train_y = labels[:int(float(len(labels))*split)]\n",
    "\n",
    "validation_x = data[int(float(len(data))*split)+1:]\n",
    "validation_y = labels[int(float(len(labels))*split)+1:]\n",
    "\n",
    "print(len(train_x), len(validation_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "G8o11BiHM1TR",
    "outputId": "a8f16e6b-7314-49d0-f659-6b698dc88e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0616 05:22:41.329031 139727582562176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is:  (778, 4)\n",
      "Training:  5-lstm-128-nodes-64-batch-20-epochs-0.3-dropout-1560662561\n",
      "Train on 80016 samples, validate on 20003 samples\n",
      "Epoch 1/20\n",
      "   64/80016 [..............................] - ETA: 2:03:37 - loss: 1.0502 - acc: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 05:22:53.403532 139727582562176 callbacks.py:257] Method (on_train_batch_end) is slow compared to the batch update (0.411164). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80016/80016 [==============================] - 365s 5ms/sample - loss: 0.7885 - acc: 0.5309 - val_loss: 0.6848 - val_acc: 0.5632\n",
      "Epoch 2/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.7028 - acc: 0.5526 - val_loss: 0.6713 - val_acc: 0.5901\n",
      "Epoch 3/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6819 - acc: 0.5687 - val_loss: 0.6640 - val_acc: 0.6000\n",
      "Epoch 4/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6714 - acc: 0.5840 - val_loss: 0.6608 - val_acc: 0.5992\n",
      "Epoch 5/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6665 - acc: 0.5938 - val_loss: 0.6631 - val_acc: 0.5901\n",
      "Epoch 6/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6623 - acc: 0.5966 - val_loss: 0.6550 - val_acc: 0.6079\n",
      "Epoch 7/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6596 - acc: 0.6015 - val_loss: 0.6654 - val_acc: 0.5944\n",
      "Epoch 8/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6579 - acc: 0.6046 - val_loss: 0.6539 - val_acc: 0.6090\n",
      "Epoch 9/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6556 - acc: 0.6077 - val_loss: 0.6538 - val_acc: 0.6091\n",
      "Epoch 10/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6544 - acc: 0.6081 - val_loss: 0.6584 - val_acc: 0.6036\n",
      "Epoch 11/20\n",
      "80016/80016 [==============================] - 360s 5ms/sample - loss: 0.6529 - acc: 0.6088 - val_loss: 0.6596 - val_acc: 0.5987\n",
      "Epoch 12/20\n",
      "80016/80016 [==============================] - 361s 5ms/sample - loss: 0.6522 - acc: 0.6093 - val_loss: 0.6619 - val_acc: 0.5955\n",
      "Epoch 13/20\n",
      "80016/80016 [==============================] - 361s 5ms/sample - loss: 0.6499 - acc: 0.6130 - val_loss: 0.6474 - val_acc: 0.6109\n",
      "Epoch 14/20\n",
      "80016/80016 [==============================] - 361s 5ms/sample - loss: 0.6503 - acc: 0.6115 - val_loss: 0.6494 - val_acc: 0.6067\n",
      "Epoch 15/20\n",
      "80016/80016 [==============================] - 361s 5ms/sample - loss: 0.6486 - acc: 0.6134 - val_loss: 0.6514 - val_acc: 0.6142\n",
      "Epoch 16/20\n",
      "80016/80016 [==============================] - 360s 5ms/sample - loss: 0.6479 - acc: 0.6147 - val_loss: 0.6520 - val_acc: 0.6024\n",
      "Epoch 17/20\n",
      "80016/80016 [==============================] - 360s 5ms/sample - loss: 0.6469 - acc: 0.6149 - val_loss: 0.6838 - val_acc: 0.5800\n",
      "Epoch 18/20\n",
      "80016/80016 [==============================] - 360s 5ms/sample - loss: 0.6466 - acc: 0.6146 - val_loss: 0.6453 - val_acc: 0.6165\n",
      "Epoch 19/20\n",
      "80016/80016 [==============================] - 360s 5ms/sample - loss: 0.6446 - acc: 0.6169 - val_loss: 0.6423 - val_acc: 0.6193\n",
      "Epoch 20/20\n",
      "80016/80016 [==============================] - 360s 4ms/sample - loss: 0.6449 - acc: 0.6181 - val_loss: 0.6460 - val_acc: 0.6141\n",
      "Copying file:///content/logs/5-lstm-128-nodes-64-batch-20-epochs-0.3-dropout-1560662561/events.out.tfevents.1560662565.0174e4453d9d [Content-Type=application/octet-stream]...\n",
      "Copying file:///content/logs/5-lstm-128-nodes-64-batch-20-epochs-0.3-dropout-1560662561/events.out.tfevents.1560662573.0174e4453d9d.profile-empty [Content-Type=application/octet-stream]...\n",
      "Copying file:///content/logs/5-lstm-128-nodes-64-batch-20-epochs-0.3-dropout-1560662561/plugins/profile/2019-06-16_05-22-53/local.trace [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 3 objects/8.0 MiB.                                      \n",
      "copied logs to Google Cloud Storage\n"
     ]
    }
   ],
   "source": [
    "import time # for naming\n",
    "\n",
    "# create more constants\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# import all the tensorflow stuff we need\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, Flatten, BatchNormalization # batch normalization layer normalizes data\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint # all the callbacks we will use\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "!mkdir models\n",
    "\n",
    "print(\"input size is: \", train_x.shape[1:])\n",
    "# ===================================== Model ============================================\n",
    "\n",
    "lstm_layers = [4] # only need to continue training from second iteration of LSTM layer here.\n",
    "nodes = [128] # try higher node number\n",
    "batch_size = [64] # try decreasing batch size, and increasing datapoints\n",
    "decayrate = 1e-6\n",
    "learningrate = 0.0001\n",
    "dropout = 0.3\n",
    "\n",
    "for node_num in nodes:\n",
    "    for layers in lstm_layers:\n",
    "        for bs in batch_size:\n",
    "            model = Sequential()\n",
    "            NAME = f\"{layers+1}-lstm-{node_num}-nodes-{bs}-batch-{EPOCHS}-epochs-{dropout}-dropout-{int(time.time())}\"\n",
    "            print(\"Training: \", NAME)\n",
    "            for i in range(layers):\n",
    "                model.add(CuDNNLSTM(node_num, input_shape=(None, train_x.shape[2]), return_sequences=True)) # 128 nodes on this layer\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "            model.add(CuDNNLSTM(node_num, input_shape=(None, train_x.shape[2]))) # can also try changing the number of nodes here... \n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "            model.add(Dense(32, activation=\"relu\"))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Dense(2, activation=\"softmax\")) # final layer.\n",
    "\n",
    "            # specify the optimizer\n",
    "            opt = tf.keras.optimizers.Adam(lr=learningrate, decay=decayrate)\n",
    "\n",
    "    \n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=opt,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "\n",
    "            history = model.fit(np.array(train_x), np.array(train_y),\n",
    "                                 batch_size=bs,\n",
    "                                 epochs=EPOCHS,\n",
    "                                 validation_data=(np.array(validation_x), np.array(validation_y)),\n",
    "                                 callbacks = [tensorboard])\n",
    "            !gsutil cp -r /content/logs/{NAME} gs://grab-ai-for-sea/logs\n",
    "            print(\"copied logs to Google Cloud Storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "PSnv-9TymHd0",
    "outputId": "1799b188-51b8-4928-8b71-c822bd600841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 15\n",
    "\n",
    "print(model.predict_classes(np.array([validation_x[index]])))\n",
    "print(validation_y[index])\n",
    "\n",
    "model.save(\"grab_ai_safety_accel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "EGYZ5bQarFmh",
    "outputId": "2af6e2e0-e3f8-4444-feb9-3dd681dc9bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/grab_ai_safety_accel.h5 [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  7.0 MiB/  7.0 MiB]                                                \n",
      "Operation completed over 1 objects/7.0 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/grab_ai_safety_accel.h5 gs://grab-ai-for-sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHIp0pfI2XqL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_train_from_pickle.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
